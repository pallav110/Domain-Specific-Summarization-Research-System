================================================================================
       DOMAIN-SPECIFIC ABSTRACTIVE SUMMARIZATION RESEARCH SYSTEM
                     Full Project Explanation Script
================================================================================

This document is a presentation/viva script. It covers everything about the
project — what it is, why it matters, how every component works, the models,
the evaluation metrics, and the three novel research contributions.

================================================================================
                         1. INTRODUCTION & MOTIVATION
================================================================================

Good morning/afternoon. I'll be presenting my research project — a Domain-
Specific Abstractive Summarization Research System.

The core research question is:

    "Does domain-specific NLP significantly improve summarization quality
     over generic models, particularly in specialized domains like legal
     and medical text?"

This is an important question because generic summarization models — trained
on news articles — often struggle with specialized vocabulary and document
structures found in legal contracts, court opinions, clinical notes, and
medical research papers. A legal document might contain clauses, obligations,
and jurisdiction-specific terminology. A medical document might reference
diagnoses, medications, dosages, and treatment plans. Generic models can
miss these critical domain elements.

To answer this question, I built a full-stack research platform that:
  - Accepts PDF and text document uploads
  - Automatically detects the document's domain (legal, medical, or general)
  - Generates summaries using six different models (two generic, two domain-
    specific, and two LLM baselines)
  - Evaluates every summary with seven quantitative metrics
  - Lets researchers compare models side by side with charts and tables
  - Exports results for external statistical analysis

The system is built with a FastAPI backend (Python) and a Next.js 16 frontend
(React 19), connected via REST APIs. All NLP models are loaded locally using
PyTorch and Hugging Face Transformers, except the LLM baselines which use
cloud APIs.


================================================================================
                        2. SYSTEM ARCHITECTURE
================================================================================

The system follows a standard client-server architecture:

    Frontend (Next.js 16 / React 19)
        |
        | REST API (HTTP)
        |
    Backend (FastAPI / Python)
        |
        |--- Document Processor (PDF/text extraction)
        |--- Domain Classifier (zero-shot + keywords)
        |--- Summarization Engine (6 models)
        |--- Evaluation Engine (7 metrics)
        |--- Novelty Modules (3 novel contributions)
        |--- SQLite Database (async via aiosqlite)

FRONTEND TECH STACK:
  - Next.js 16.1.6 with Turbopack bundler
  - React 19.2.4
  - Tailwind CSS + shadcn/ui (Radix primitives)
  - Recharts for data visualization
  - react-pdf 10.x for inline PDF viewing
  - Axios for HTTP communication

BACKEND TECH STACK:
  - FastAPI with async SQLAlchemy
  - SQLite database (async via aiosqlite)
  - PyTorch + Hugging Face Transformers
  - Sentence-Transformers for embeddings
  - rouge-score, bert-score libraries
  - Google Gemini API (free tier)
  - Loguru for logging

DATABASE SCHEMA:
  Four main tables:
  - Document: stores uploaded files, extracted text, domain classification
  - Summary: stores generated summaries with model metadata
  - Evaluation: stores all computed metrics for each summary
  - Experiment: tracks research experiments across multiple documents


================================================================================
                       3. DOCUMENT PROCESSING PIPELINE
================================================================================

When a user uploads a document, the following pipeline executes:

STEP 1: TEXT EXTRACTION
  - For PDFs: Uses pdfplumber (primary) with PyPDF2 fallback
    - pdfplumber handles complex layouts, tables, and multi-column PDFs
  - For TXT files: Standard UTF-8 reading
  - Text is cleaned: excessive whitespace removed, special characters
    stripped, PDF hyphenation artifacts fixed

STEP 2: DOMAIN CLASSIFICATION
  The system automatically detects whether the document is legal, medical,
  or general using a hybrid approach:

  PRIMARY — Zero-Shot Classification:
    - Model: facebook/bart-large-mnli (BART trained on Multi-Genre NLI)
    - Candidate labels: "legal document", "medical document", "general document"
    - Uses the first 1000 words for efficiency
    - Returns probability scores for each label

  AUXILIARY — Keyword Analysis:
    - 25+ legal keywords: contract, plaintiff, defendant, jurisdiction,
      pursuant, covenant, liability, indemnify, statute, etc.
    - 25+ medical keywords: patient, diagnosis, treatment, symptoms,
      prescription, pathology, cardiovascular, neurological, etc.
    - Computes keyword density ratio for each domain

  COMBINED SCORE:
    legal_score  = 0.7 x zero_shot_legal  + 0.3 x keyword_ratio_legal
    medical_score = 0.7 x zero_shot_medical + 0.3 x keyword_ratio_medical

  DECISION:
    If legal > medical AND legal > 0.25  -->  domain = "legal"
    If medical > legal AND medical > 0.25 -->  domain = "medical"
    Otherwise                              -->  domain = "unknown"

  The confidence score is stored alongside the classification.

STEP 3: METADATA STORAGE
  - Word count, file size, file type, domain, confidence
  - Raw text stored in the database for summarization
  - Original file preserved on disk for PDF viewing


================================================================================
                       4. SUMMARIZATION MODELS (6 TOTAL)
================================================================================

The system compares six summarization models across three categories.

------------ CATEGORY A: GENERIC MODELS (2 models) ------------

MODEL 1: BART (facebook/bart-large-cnn)
  - Architecture: Denoising autoencoder (encoder-decoder Transformer)
  - Pre-training: Corrupted text reconstruction on large corpus
  - Fine-tuning: CNN/DailyMail news summarization dataset
  - Parameters: ~406 million
  - Generation settings:
      max_length = 512, min_length = 100
      num_beams = 4 (beam search decoding)
      length_penalty = 2.0 (encourages concise output)
      early_stopping = True
  - Input limit: 1024 tokens (truncates longer documents)
  - Speed: ~3 seconds per summary

MODEL 2: PEGASUS (google/pegasus-cnn_dailymail)
  - Architecture: Encoder-decoder Transformer
  - Pre-training innovation: Gap Sentence Generation (GSG) — masks entire
    important sentences during pre-training and learns to reconstruct them
  - This pre-training objective is specifically designed for summarization
  - Fine-tuning: CNN/DailyMail dataset
  - Same generation parameters as BART
  - Speed: ~3 seconds per summary
  - Post-processing: Removes PEGASUS-specific <n> newline tokens


------------ CATEGORY B: DOMAIN-SPECIFIC MODELS (2 models) ------------

These use a novel TWO-STAGE PIPELINE: extractive first, then abstractive.

MODEL 3: Legal-BERT + PEGASUS
  Stage 1 — Extractive (Legal-BERT):
    - Model: nlpaueb/legal-bert-base-uncased
    - This is BERT pre-trained on 12GB of legal text (EU legislation,
      court cases, contracts) — understands legal terminology natively
    - Process:
      1. Split document into sentences (minimum 10 characters each)
      2. Feed each sentence through Legal-BERT
      3. Extract the [CLS] token embedding (768-dimensional vector)
      4. Compute document centroid: mean of ALL sentence embeddings
      5. Score each sentence by cosine similarity to centroid
         (higher score = more representative of document)
      6. Select top 60% of sentences by score (minimum 3 sentences)
      7. Reconstruct extracted text preserving original order

  Stage 2 — Abstractive (PEGASUS):
    - Feed the extracted key sentences into PEGASUS
    - PEGASUS generates a fluent abstractive summary from the extraction
    - Same beam search parameters as the standalone PEGASUS

  WHY THIS WORKS: Legal-BERT embeddings capture domain-specific semantics
  that generic BERT would miss. The extractive stage filters for legally
  relevant content (obligations, parties, terms) before PEGASUS generates
  a readable abstract. This prevents the abstractive model from being
  overwhelmed by boilerplate or irrelevant sections.

  Speed: ~38 seconds (two-stage process)

MODEL 4: Clinical-BERT + PEGASUS
  - Identical two-stage architecture
  - Stage 1 uses: emilyalsentzer/Bio_ClinicalBERT
    - BERT pre-trained on MIMIC-III clinical notes (2 million+ notes)
    - Understands medical terminology: diagnoses, medications, vital signs,
      treatment plans, lab results
  - Stage 2: Same PEGASUS abstraction
  - Speed: ~38 seconds


------------ CATEGORY C: LLM BASELINES (2 models) ------------

MODEL 5: Google Gemini 1.5 Flash
  - Type: Large Language Model (cloud API)
  - API: Free tier — 15 requests per minute
  - Domain-aware prompting: The system sends different prompts based on
    the detected domain:
    - Legal prompt: "Focus on key clauses, parties, obligations, rights,
      terms, conditions, and legal implications..."
    - Medical prompt: "Focus on patient information, diagnosis, treatment
      plan, medications, dosages, and prognosis..."
    - General prompt: Standard summarization instruction
  - Parameters: temperature = 0.3 (near-deterministic), max_output = 2048
  - Advantage: Handles very long documents (128K context window)
  - Speed: ~5 seconds

MODEL 6: GPT-4 Turbo (OpenAI)
  - Type: Large Language Model (cloud API, requires paid key)
  - Same domain-aware prompting as Gemini
  - System prompt: "You are an expert document summarizer..."
  - Speed: ~8 seconds
  - Note: Optional — system works fully without it


================================================================================
                       5. EVALUATION METRICS (7 METRICS)
================================================================================

Every generated summary is evaluated against the source document using
seven complementary metrics that measure different aspects of quality.

METRIC 1: ROUGE-1 (Recall-Oriented Understudy for Gisting Evaluation)
  - Measures: Unigram (single word) overlap between summary and source
  - Computes: Precision, Recall, and F1 score
  - Precision = matching unigrams / total summary unigrams
  - Recall = matching unigrams / total source unigrams
  - F1 = harmonic mean of precision and recall
  - Range: 0 to 1 (higher is better)
  - Implementation: rouge-score library with Porter stemming

METRIC 2: ROUGE-2
  - Same as ROUGE-1 but uses bigrams (2-word sequences)
  - Captures phrase-level overlap, not just individual words
  - More strict than ROUGE-1 — a higher ROUGE-2 indicates the summary
    preserves phrasing from the source

METRIC 3: ROUGE-L
  - Based on Longest Common Subsequence (LCS)
  - Captures sentence-level structure and word ordering
  - Does not require consecutive matches (unlike bigrams)
  - Most commonly reported ROUGE metric in research papers

METRIC 4: BERTScore F1
  - Measures: Semantic similarity using contextual embeddings
  - Model: distilbert-base-uncased (for speed; deberta-xlarge-mnli for
    detailed analysis)
  - Process:
    1. Tokenize both summary and reference with BERT tokenizer
    2. Obtain contextual embeddings for each token
    3. Compute pairwise cosine similarity matrix
    4. Precision: for each summary token, find max similarity to any
       reference token, then average
    5. Recall: for each reference token, find max similarity to any
       summary token, then average
    6. F1: harmonic mean of precision and recall
  - Advantage over ROUGE: captures semantic equivalence even when
    different words are used (e.g., "attorney" vs "lawyer")
  - Range: 0 to 1 (higher is better)

METRIC 5: Factuality Score
  - Measures: Factual consistency — is the summary faithful to the source?
  - Primary method: Natural Language Inference (NLI)
    - Model: microsoft/deberta-base-mnli
    - Process:
      1. Split summary into individual sentences (max 10)
      2. For each sentence, create an NLI pair:
         Premise = source text, Hypothesis = summary sentence
      3. Run through NLI classifier
      4. Extract probability of "ENTAILMENT" label
      5. Average entailment scores across all summary sentences
    - Interpretation: High score = summary makes claims supported by source
    - A low factuality score indicates hallucination
  - Fallback method: Word overlap ratio (if NLI model unavailable)
  - Range: 0 to 1 (higher is better)

METRIC 6: Semantic Similarity
  - Measures: Overall meaning preservation
  - Model: SentenceTransformer('all-MiniLM-L6-v2')
    - Lightweight sentence embedding model (22M parameters)
    - Maps entire text to a 384-dimensional vector
  - Process:
    1. Encode full summary text -> vector A
    2. Encode full source text -> vector B
    3. Compute cosine similarity between A and B
  - Range: 0 to 1 (higher is better)
  - Captures global semantic alignment rather than word-level overlap

METRIC 7: Compression Ratio
  - Measures: How much the text was compressed
  - Formula: summary_word_count / source_word_count
  - Range: 0 to 1 (lower means more compression)
  - Ideal range: 0.05 to 0.20 for abstractive summarization
  - Important: Very low compression might lose content; very high means
    insufficient summarization

OVERALL WEIGHTED SCORE (used for model ranking):
  score = 0.30 x ROUGE-L
        + 0.30 x BERTScore_F1
        + 0.20 x Semantic_Similarity
        + 0.20 x Factuality_Score

  This weighting balances lexical overlap (ROUGE), semantic quality
  (BERTScore + Semantic Similarity), and factual accuracy (Factuality).


================================================================================
                  6. NOVEL RESEARCH CONTRIBUTIONS (3 NOVELTIES)
================================================================================

Beyond the standard comparison framework, this system introduces three
novel contributions that go beyond existing literature.


------------ NOVELTY 1: SENTENCE-LEVEL ENSEMBLE SUMMARIZATION ------------

PROBLEM: Each model produces a different summary. How do we combine them
into a single, superior summary that captures the best content from all?

Existing approaches typically pick the "best" model. Our approach FUSES
outputs at the sentence level using semantic clustering.

ALGORITHM:

  Step 1 — Extract Sentences with Provenance
    - Parse each model's summary into individual sentences
    - Track which model produced each sentence and its position

  Step 2 — Compute Semantic Embeddings
    - Encode all extracted sentences using SentenceTransformer (all-MiniLM-L6-v2)
    - Also encode the source document for relevance scoring

  Step 3 — Greedy Agglomerative Clustering
    - For each sentence, find all other sentences with cosine similarity > 0.7
    - Group them into clusters
    - Each cluster represents a single "idea" that multiple models expressed

  Step 4 — Score Each Cluster
    cluster_score = agreement_count x avg_source_relevance

    Where:
      agreement_count = number of distinct models that contributed a
                        sentence to this cluster
      avg_source_relevance = average cosine similarity of cluster
                             sentences to the source document embedding

    INTUITION: An idea is important if (a) multiple models independently
    chose to include it AND (b) it is relevant to the original source.

  Step 5 — Select and Order Representatives
    - Sort clusters by score (descending)
    - Select top-k clusters (default k = 10)
    - From each cluster, pick the representative sentence with highest
      source relevance
    - Order the selected sentences by their original position

  Output: A single ensemble summary + metadata showing:
    - How many clusters were formed and selected
    - Which source models contributed to each cluster
    - Agreement and relevance scores per cluster

WHY THIS IS NOVEL:
  Most ensemble methods simply re-rank or vote on complete summaries.
  Our approach operates at the sentence level, allowing the ensemble to
  select the best sentence for each idea from different models. This
  produces summaries that can outperform any individual model.


------------ NOVELTY 2: CROSS-MODEL CONSENSUS METRIC ------------

PROBLEM: Existing metrics (ROUGE, BERTScore, factuality) all compare a
summary against the source document. None measure whether multiple models
AGREE with each other. If 4 out of 5 models independently include the
same information, that's a strong reliability signal.

Our Cross-Model Consensus Metric measures inter-model agreement.

ALGORITHM:

  1. PAIRWISE AGREEMENT MATRIX (N x N models):
     For each pair of models A and B:
       - Split each model's summary into sentences
       - Encode all sentences with SentenceTransformer
       - For each sentence in A, find the most similar sentence in B
       - agreement[A][B] = average of these max similarities

     This creates a heatmap showing which model pairs produce similar
     content and which diverge significantly.

  2. PER-SENTENCE CONSENSUS:
     For each sentence in each model's summary:
       - Check how many OTHER models have a semantically similar sentence
         (cosine similarity > 0.7)
       - Record the "consensus count" (how many models agree)
       - Record the "consensus ratio" (consensus_count / total_models)

  3. CONSENSUS SCORE (overall):
     consensus_score = num_high_agreement_sentences / total_sentences
     Where high_agreement = consensus_count >= 2

     Interpretation:
       - Score > 0.8: Models strongly agree — high reliability
       - Score 0.5-0.8: Moderate agreement — some divergence
       - Score < 0.5: Models disagree — may need manual review

  4. UNIQUE CONTENT RATIO (per model):
     For each model: fraction of sentences that NO other model produced
       unique_ratio = unique_sentences / total_model_sentences

     Interpretation:
       - High ratio: This model captures perspectives others miss
       - Low ratio: This model's content is redundant with others

  5. HIGH-AGREEMENT SENTENCES:
     The top sentences ranked by consensus count, showing:
       - The sentence text
       - Which model produced it
       - Which other models agree
       - Agreement ratio badge (e.g., "4/5 models agree")

WHY THIS IS NOVEL:
  No standard NLP evaluation metric measures inter-model agreement.
  ROUGE and BERTScore only compare summary-to-source. Our consensus
  metric provides a new dimension of quality assessment: if models
  trained on entirely different architectures and data independently
  agree on the same content, that content is likely important and
  reliable. This is analogous to inter-rater reliability in human
  evaluation.


------------ NOVELTY 3: ADAPTIVE MODEL RECOMMENDER ------------

PROBLEM: With 6 models available, which one should a researcher use for
a NEW document? Currently, you'd have to run all models and compare.
Our recommender predicts the best model BEFORE summarization, saving
significant computation time.

ALGORITHM:

  Feature Extraction (7-dimensional vector per document):
    1. word_count — total words in the document
    2. sentence_count — number of sentences
    3. avg_sentence_length — word_count / sentence_count
    4. domain_confidence — classifier confidence (0.0 to 1.0)
    5. type_token_ratio — unique_words / total_words (vocabulary richness)
    6. is_legal — binary flag (1.0 if legal domain, else 0.0)
    7. is_medical — binary flag (1.0 if medical domain, else 0.0)

  Training (from historical evaluation data):
    1. Query all past evaluations from the database
    2. For each document, compute weighted score per model:
       score = 0.3 x ROUGE-L + 0.3 x BERTScore + 0.2 x Semantic + 0.2 x Fact
    3. Label = model with the highest weighted score for that document
    4. Build training set: {features, best_model} pairs

    IF sklearn is available:
      - Train a RandomForestClassifier
      - n_estimators = 50 trees, max_depth = 5
      - Returns class probabilities for all models
      - Minimum 3 training samples required

    IF sklearn is NOT available (fallback):
      - Use numpy-based k-Nearest Neighbors (k=3)
      - L2 distance on z-score normalized features
      - Majority vote among 3 nearest neighbors

  Prediction:
    - Extract features from the new document
    - Feed through trained classifier
    - Return: recommended model, confidence, all model probabilities
    - The recommended model is auto-selected in the frontend

  Rule-Based Fallback (when no training data exists):
    - Legal document with confidence > 40%  -->  Legal-BERT + PEGASUS
    - Medical document with confidence > 40% -->  Clinical-BERT + PEGASUS
    - Document with 3000+ words             -->  Gemini (long context)
    - Otherwise                              -->  BART (reliable baseline)

WHY THIS IS NOVEL:
  Existing summarization systems treat model selection as a manual choice.
  Our adaptive recommender turns it into a machine learning problem,
  learning from historical performance data which model works best for
  which type of document. As more documents are processed and evaluated,
  the recommender becomes more accurate. This is a meta-learning approach
  to summarization model selection.


================================================================================
                     7. INTERACTIVE PDF VIEWER WITH HIGHLIGHTS
================================================================================

The system includes an interactive PDF viewer that displays the original
uploaded document with color-coded highlights showing which parts of the
source text each model's summary drew from.

HOW IT WORKS:
  1. Extract n-grams (4-word sequences) from each model's summary
  2. Search for these n-grams in the PDF's text layer
  3. Highlight matching spans with the model's assigned color

COLOR CODING:
  Orange  = BART
  Violet  = PEGASUS
  Blue    = Gemini
  Green   = GPT-4
  Cyan    = Legal-BERT + PEGASUS
  Pink    = Clinical-BERT + PEGASUS
  Gold    = Ensemble (Novelty 1)

Users can toggle which models' highlights are visible, allowing them to
visually verify that summaries are grounded in the source document and
compare which sections different models focused on.


================================================================================
                       8. FRONTEND PAGES (9 PAGES)
================================================================================

1. HOME (/)
   Quick upload area, recent documents, system overview

2. DOCUMENTS (/documents)
   Browse all uploaded documents, filter by domain, see word counts

3. DOCUMENT DETAIL (/documents/[id])
   - Document metadata (word count, domain, confidence)
   - Model Recommendation panel (Novelty 3) — shows recommended model
   - Model selector with checkboxes to choose which models to run
   - Generated summaries table
   - PDF Viewer tab with color-coded highlights

4. SUMMARY DETAIL (/summaries/[id])
   Full summary text with all evaluation metrics

5. COMPARE MODELS (/compare/[id])
   - Full metrics table with all 7 metrics per model
   - Precision/Recall toggle for detailed breakdown
   - Consensus button (Novelty 2) — shows agreement matrix heatmap,
     unique content ratio chart, top consensus sentences
   - Ensemble button (Novelty 1) — generates and displays fused summary
   - Bar charts: summary length, generation time, ROUGE scores
   - Radar chart: multi-metric visual comparison
   - Side-by-side summary text with model color coding

6. EXPERIMENTS (/experiments)
   Create and manage reproducible research experiments

7. RESULTS (/results)
   Filterable table of all evaluation results across all documents

8. STATISTICS (/statistics)
   Per-model and per-domain statistical analysis with averages and
   standard deviations

9. DASHBOARD (/dashboard)
   Aggregate statistics, charts, domain distribution, model usage trends


================================================================================
                       9. API ENDPOINTS (17 ENDPOINTS)
================================================================================

DOCUMENTS:
  POST /api/v1/documents/upload       Upload PDF/TXT file
  GET  /api/v1/documents              List all documents
  GET  /api/v1/documents/{id}         Get document details
  GET  /api/v1/documents/{id}/file    Download original file
  DELETE /api/v1/documents/{id}       Delete document

SUMMARIZATION:
  POST /api/v1/summarize              Generate summaries for selected models
  GET  /api/v1/summaries/{id}         Get summary by ID
  GET  /api/v1/documents/{id}/summaries  List summaries for a document

EVALUATION:
  POST /api/v1/evaluate/{summary_id}  Run evaluation metrics on a summary
  GET  /api/v1/evaluations/summary/{id}  Get evaluation results

ANALYSIS:
  GET  /api/v1/compare/{document_id}  Compare all models on a document
  GET  /api/v1/dashboard/stats        Aggregate dashboard statistics
  GET  /api/v1/research/results       Detailed results for analysis
  GET  /api/v1/statistics/analysis    Per-model statistical analysis

NOVELTY ENDPOINTS:
  POST /api/v1/ensemble/{document_id}   Generate ensemble summary (Novelty 1)
  GET  /api/v1/consensus/{document_id}  Compute consensus metrics (Novelty 2)
  GET  /api/v1/recommend/{document_id}  Get model recommendation (Novelty 3)
  POST /api/v1/recommender/train        Train recommender from history (Novelty 3)

EXPORT:
  GET  /api/v1/export/csv             Export all results as CSV
  GET  /api/v1/export/json            Export all results as JSON


================================================================================
                  10. EXPERIMENTAL METHODOLOGY & USAGE
================================================================================

RECOMMENDED EXPERIMENT DESIGN:

  1. DATA COLLECTION
     - Upload at least 10 documents per domain (20+ total)
     - Mix of legal and medical documents
     - Varying lengths and complexity

  2. SUMMARY GENERATION
     - Generate summaries with all 6 models for each document
     - Use consistent parameters across all runs

  3. EVALUATION
     - Run evaluation on every generated summary
     - All 7 metrics computed automatically

  4. COMPARISON
     - Use the Compare page for per-document model comparison
     - Use Statistics page for aggregate per-model averages and std devs
     - Use Consensus (Novelty 2) to check inter-model reliability
     - Use Ensemble (Novelty 1) to generate fused summaries

  5. STATISTICAL ANALYSIS
     - Export CSV for analysis in Python or R
     - Perform paired t-tests between model pairs
     - Example:
         from scipy.stats import ttest_rel
         t_stat, p_val = ttest_rel(bart_scores, legal_bert_scores)

  6. TRAIN RECOMMENDER
     - After sufficient evaluations (3+ documents), train the recommender
     - POST /api/v1/recommender/train
     - Future documents will receive ML-based recommendations


================================================================================
                       11. KEY FINDINGS & EXPECTED RESULTS
================================================================================

Based on the system design and NLP literature, expected findings:

  1. Domain-specific models (Legal-BERT + PEGASUS, Clinical-BERT + PEGASUS)
     are expected to outperform generic models on in-domain documents,
     particularly on BERTScore and semantic similarity metrics.

  2. Generic models (BART, PEGASUS) may perform comparably on general
     documents or short specialized documents where domain knowledge
     matters less.

  3. LLM baselines (Gemini, GPT-4) may achieve competitive or superior
     results across all domains due to their large scale and instruction-
     following ability, but at higher computational cost.

  4. The ensemble (Novelty 1) is expected to match or exceed the best
     individual model by combining strengths from multiple models.

  5. The consensus metric (Novelty 2) is expected to correlate with
     human judgments of importance — sentences that multiple models
     independently include are likely truly important.

  6. The recommender (Novelty 3) should learn to route legal documents
     to Legal-BERT and medical documents to Clinical-BERT, with
     accuracy improving as more training data accumulates.


================================================================================
                       12. TECHNICAL REQUIREMENTS
================================================================================

  - Python 3.9+
  - Node.js 18+
  - CUDA-compatible GPU recommended (not required — CPU mode available)
  - ~5GB disk space for model downloads (cached after first run)
  - Google Gemini API key (free at https://makersuite.google.com/app/apikey)
  - OpenAI API key (optional, for GPT-4)

MODELS DOWNLOADED AUTOMATICALLY:
  - facebook/bart-large-cnn (~1.6GB)
  - google/pegasus-cnn_dailymail (~2.2GB)
  - nlpaueb/legal-bert-base-uncased (~440MB)
  - emilyalsentzer/Bio_ClinicalBERT (~440MB)
  - facebook/bart-large-mnli (~1.6GB) — domain classifier
  - microsoft/deberta-base-mnli (~700MB) — factuality
  - sentence-transformers/all-MiniLM-L6-v2 (~80MB) — embeddings

All models are cached in ./models_cache after first download.


================================================================================
                            13. CONCLUSION
================================================================================

This project makes three contributions:

  1. A comprehensive comparison framework for generic vs domain-specific
     abstractive summarization across legal and medical domains.

  2. Three novel research features:
     - Sentence-Level Ensemble Summarization: fuses multiple model outputs
       into a single superior summary using semantic clustering
     - Cross-Model Consensus Metric: a new evaluation dimension measuring
       inter-model agreement as a reliability signal
     - Adaptive Model Recommender: ML-based prediction of the best model
       for a given document, learning from historical performance

  3. A production-quality full-stack research platform that enables
     reproducible experiments with interactive visualization, PDF viewing
     with model highlights, and data export for statistical analysis.

The system demonstrates that domain-specific models can provide meaningful
improvements in specialized domains, while also introducing novel tools
for ensemble generation, reliability assessment, and intelligent model
selection that advance the state of summarization research.

================================================================================
                          END OF SCRIPT
================================================================================
